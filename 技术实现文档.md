# 多智能体SRS生成系统技术实现文档

## 概述

本文档详细描述基于LangGraph的多智能体SRS（软件需求规格说明）生成系统中各个智能体的技术实现细节。系统包含5个核心智能体，通过状态流转和迭代优化，将自然语言需求转换为符合IEEE Std 830-1998标准的SRS文档。

## 系统架构

系统采用LangGraph构建状态图，流程如下：
- **ReqParse** → **ReqExplore** ↔ **ReqClarify** → **DocGenerate** → END
- 支持消融实验模式：`no-clarify`、`no-explore-clarify`

---

## （一）需求解析智能体（ReqParse）

### 输入

- **状态字段**：
  - `state["user_input"]`：自然语言需求文本（字符串）
  - `state["iteration"]`：当前迭代轮数（整数，初始为0）

- **LLM配置**：
  - 模型：从环境变量 `OPENAI_MODEL` 读取（默认：`gpt-4o-mini`）
  - 温度：`0.2`（可通过 `OPENAI_TEMP_REQPARSE` 环境变量覆盖）
  - 重试次数：最多3次

### Prompt设计

```python
'You are the "Requirement Parsing Agent (ReqParse)", responsible for converting natural language requirements into an **atomic** initial list.

[Parsing Method (done internally, do not appear in output)]
1) Identify triples: <Actor/Role, Action, Object>, and map to business objects (e.g., user/admin/system/external service).
2) Atomic splitting: Each requirement describes only **a single verifiable behavior or constraint**; when encountering conjunctions like "and/also/or/simultaneously", split into multiple items if necessary.
3) Deduplication and synonym merging: Merge equivalent expressions, unify terminology and units; avoid repetition.
4) Classification (heuristic):
   • FR-* (Functional): Describes observable behaviors or interface interactions of input→processing→output.
   • NFR-* (Non-functional): Performance/capacity (throughput/concurrency/response latency/peak), reliability/availability, maintainability, portability,
     security (authentication/audit/minimum privilege/encryption/compliance), observability (logs/metrics/tracing).
   • CON-* (Constraint): Legal/compliance/organizational policies/platform boundaries/external dependencies/data sovereignty/deployment and network constraints, etc., which are **strong constraints** not primarily focused on functional experience.
5) Terminology anchoring:
   • Preserve proper nouns and units from user input; prohibit inventing new terms; if nouns need to be supplemented, use "Prerequisite: ..." embedded at the end of content.
6) Quality threshold: Avoid vague words like "possibly/try to/appropriate/TBD/?"; testable, traceable; prohibit question-style/clarification-style output.

[Numbering Convention]
- Functional requirements: FR-01, FR-02, FR-03, ... (two-digit increment, starting from 01, consecutive and unique)
- Non-functional requirements: NFR-01, NFR-02, ... (two-digit increment)
- Constraints: CON-01, CON-02, ... (two-digit increment)
Numbers must not skip or repeat; the three sequences are independent.

[Output Format (JSON array only, elements contain only id and content)]
```json
[{"id":"FR-01","content":"……"}]
```
No fields other than id and content may appear; no explanations, tables, or comments may be output.

---

User Requirements:
{user_input}

Please output only a structured JSON array (outer layer uses ```json fence, array elements contain only id and content).'
```

**Prompt特点**：
- 强调原子性拆分：每个需求描述单一可验证行为
- 三元组识别：<角色，动作，对象>
- 分类规则：FR（功能）、NFR（非功能）、CON（约束）
- 编号规范：FR-01/NFR-01/CON-01格式，两位数递增
- 术语锚定：保留用户输入中的专有名词和单位

### 操作：ReqParse

**函数位置**：`srs_generator_v1.py:293-358`

**执行流程**：

1. **初始化LLM**：
   ```python
   llm = get_llm_for("ReqParse")  # temperature=0.2
   ```

2. **构建Prompt**：
   - 使用 `user_template.format(user_input=state["user_input"])` 填充模板

3. **调用LLM并解析JSON**：
   - 使用 `invoke_with_json_retry(llm, messages, max_retries=3)` 调用
   - 自动重试机制：最多3次，指数退避（2^attempt秒）
   - JSON提取：优先匹配 ````json ... ``` 代码围栏，失败则尝试裸JSON

4. **数据规范化**：
   ```python
   normalized: List[Dict[str, str]] = []
   for it in parsed:
       rid = str(it.get("id"))
       content = str(it.get("content", "")).strip()
       if not rid or not content:
           continue  # 跳过无效项
       normalized.append({"id": rid, "content": content})
   ```

5. **记录交互日志**：
   - 调用 `record_llm_interaction()` 记录输入消息、原始输出、解析后输出

6. **更新状态**：
   - `state["req_list"] = normalized`
   - `state["iteration"] = current_iteration`（递增）

### 输出

- **状态更新**：
  - `state["req_list"]`：初始需求清单（JSON数组）
    - 格式：`[{"id": "FR-01", "content": "..."}, ...]`
    - 仅包含 `id` 和 `content` 字段
    - ID格式：FR-XX、NFR-XX、CON-XX（两位数编号）
  
- **日志记录**：
  - `state["logs"]` 中追加一条记录，包含：
    - `iteration`：迭代轮数
    - `agent`：`"ReqParse"`
    - `input_messages`：发送给LLM的消息
    - `raw_output`：LLM原始输出
    - `parsed_output`：解析后的JSON数组

- **控制台输出**：
  - `[日志] ReqParse：开始解析用户输入`
  - `[日志] ReqParse：解析完成，共 N 条需求`

---

## （二）需求挖掘智能体（ReqExplore）

### 输入

- **状态字段**：
  - `state["req_list"]`：当前需求清单（JSON数组）
  - `state["scores"]`：需求评分字典 `{id: score}`
  - `state["frozen_ids"]`：已冻结需求ID列表（只读，不参与处理）
  - `state["frozen_reqs"]`：冻结需求内容字典 `{id: content}`
  - `state["removed_ids"]`：已移除需求ID列表（只读，不参与处理）
  - `state["iteration"]`：当前迭代轮数

- **LLM配置**：
  - 模型：从环境变量 `OPENAI_MODEL` 读取（默认：`gpt-4o-mini`）
  - 温度：`0.6`（可通过 `OPENAI_TEMP_REQEXPLORE` 环境变量覆盖）
  - 重试次数：最多3次

### Prompt设计

```python
'You are the "Requirement Exploration Agent (ReqExplore)", a senior requirements engineer and closed-loop design expert.
You must perform **aggressive but controlled expansion**, only on **unfrozen and non-removed** items.
Frozen items and removed items are strictly read-only and must NOT appear in your output.

[Your Goals]
1) For each unfrozen requirement, use its score and content to:
   - Improve clarity, testability, and structure.
   - Systematically derive ALL reasonably implied supporting requirements
     (functional, non-functional, constraints), as long as they:
       • Stay within the SAME business/domain scope;
       • Are strongly grounded in existing items or Reference SRS patterns;
       • Make the end-to-end behavior verifiable and operable.
2) Compared to a conservative mode, you are encouraged to generate MORE items
   when they are logically necessary, typical, or high-confidence implications.

[Score-Based Exploration Strategy]
+2 (Strong Adoption):
   - Keep semantics strictly; refine wording/structure.
   - MAY add missing **fine-grained checks** tightly coupled to this item
     (e.g., explicit error handling, logging, audit, boundary cases)
     when obviously implied; avoid changing its scope.
+1 (Adoption):
   - Keep the core intent.
   - AGGRESSIVELY enumerate supporting requirements needed to test/operate it:
       • detailed input/output rules;
       • role/permission & audit;
       • exception handling & rollback;
       • data integrity & idempotency;
       • observability (logs/metrics/traces/alerts).
   - You MAY generate multiple new items around this requirement to cover the loop.
0 (Neutral):
   - Treat as a seed: clarify assumptions via verifiable expressions.
   - Split vague or overloaded text into multiple precise items.
   - Derive variants/scenarios (normal / abnormal / edge cases) when reasonable.
-1 (Non-adoption):
   - Rewrite or replace into a set of precise items that match scope & constraints.
   - If one vague item actually implies several concrete responsibilities,
     expand it into multiple new requirements.
-2 (Strong Non-adoption):
   - This id is handled outside you: DO NOT output or resurrect it,
     and DO NOT produce near-synonym replacements for it.

[Aggressive Exploration Boundaries]
- You MAY create many new FR/NFR/CON/SUG items when:
   • They are direct upstream/downstream/operational/support requirements of existing items; or
   • They reflect standard, high-confidence practices (security, audit, monitoring, reliability)
     required to make current requirements pass real-world acceptance.
- You MUST NOT:
   • Introduce a completely new business domain/module unrelated to current list;
   • Invent new roles or external systems out of thin air;
   • Reintroduce any id from Removed List, or its near-synonyms.

[Numbering Rules]
- Only output **unfrozen + new** items.
- Do NOT output frozen ids and removed ids.
- For existing unfrozen ids: keep their ids, update contents as needed.
- For new items:
   • Use id prefixes FR-/NFR-/CON-/SUG- consistent with their nature;
   • Ensure no conflict with any existing id;
   • Continue sequences at the tail (you infer next numbers from context).

[Closed-Loop Checklist for Each Area]
- Trigger & input conditions are explicit.
- Processing steps and decision rules are clear.
- Output and acceptance criteria are testable.
- Error/exception handling & rollback paths exist.
- Access control & audit trail are covered.
- Data consistency & idempotency are addressed when relevant.
- Observability: logs/metrics/traces/alerts for critical flows.
- Prerequisites are explicit via "Prerequisite: ..." in content if needed.
- Remove vague words (fast/many/robust/etc.) by adding thresholds.

[Hard Output Constraints]
- You MUST return ONLY one JSON array wrapped by ```json ...```.
- Each element: {"id":"...","content":"..."}.
- No extra keys, no comments, no natural-language explanation.
- Do NOT include any frozen id or removed id.

[(Read-only) Frozen List]:
{frozen_payload_json}

[(Read-only) Removed List (MUST NOT be mentioned/reintroduced)]:
{removed_payload_json}

---

Unfrozen & Non-removed List (JSON):
```json
{unfrozen_list_json}
```

Scores (for unfrozen & non-removed items, JSON):
```json
{scores_arr_unfrozen_json}
```

Now output the optimized + aggressively expanded list as a single JSON array (```json fenced).'
```

**Prompt特点**：
- 基于评分的差异化挖掘策略（+2到-2）
- 强调闭环设计检查清单（触发条件、处理步骤、异常处理等）
- 严格控制边界：不引入新业务域，不创造新角色
- 冻结和移除项严格只读，不出现在输出中

### 操作：ReqExplore

**函数位置**：`srs_generator_v1.py:361-559`

**执行流程**：

1. **初始化LLM**：
   ```python
   llm = get_llm_for("ReqExplore")  # temperature=0.6
   ```

2. **过滤未冻结且未移除的需求**：
   ```python
   frozen_ids_set = set(state["frozen_ids"])
   removed_ids_set = set(state["removed_ids"])
   unfrozen_list = [
       it for it in state["req_list"]
       if it["id"] not in frozen_ids_set and it["id"] not in removed_ids_set
   ]
   ```

3. **提前退出检查**：
   - 如果 `unfrozen_list` 为空，跳过LLM调用，直接返回状态

4. **构建Prompt**：
   - 序列化冻结列表、移除列表、未冻结列表、评分数组为JSON字符串
   - 使用 `user_template.format()` 填充模板

5. **调用LLM并解析JSON**：
   - 使用 `invoke_with_json_retry(llm, messages, max_retries=3)`

6. **合并逻辑**（关键）：
   ```python
   # 1. 构建新内容映射（过滤越界输出）
   new_map: Dict[str, str] = {}
   for it in unfrozen_new_list:
       rid = str(it.get("id"))
       if not rid or rid in frozen_ids_set or rid in removed_ids_set:
           continue  # 丢弃越界输出
       content = str(it.get("content", "")).strip()
       if content:
           new_map[rid] = content
   
   # 2. 按原顺序合并：保留冻结项、更新未冻结项、追加新增项
   merged: List[Dict[str, str]] = []
   prev_ids_order = [str(it["id"]) for it in state["req_list"]]
   
   for rid in prev_ids_order:
       if rid in removed_ids_set:
           continue  # 跳过移除项
       if rid in frozen_ids_set:
           # 冻结项：使用frozen_reqs中的内容
           content = state["frozen_reqs"].get(rid, ...)
           merged.append({"id": rid, "content": content})
       else:
           # 未冻结项：使用新内容或保留旧内容
           content = new_map.get(rid, ...)
           merged.append({"id": rid, "content": content})
   
   # 3. 追加真正新增的条目
   for rid, content in new_map.items():
       if rid not in prev_ids_order and rid not in removed_ids_set:
           merged.append({"id": rid, "content": content})
   ```

7. **更新状态**：
   - `state["req_list"] = merged`

### 输出

- **状态更新**：
  - `state["req_list"]`：优化后的需求清单
    - 保留冻结项（内容不变）
    - 更新未冻结项（根据LLM输出）
    - 追加新增项（LLM生成的新需求）
    - 排除移除项（不包含在列表中）

- **日志记录**：
  - `state["logs"]` 中追加一条记录

- **控制台输出**：
  - `[日志] ReqExplore：第 X 轮，根据评分强化挖掘与优化（仅处理未冻结且未移除项）`
  - `[日志] ReqExplore：强化挖掘完成，共 N 条；冻结 M 条；已移除 K 条`

---

## （三）需求澄清智能体（ReqClarify）

### 输入

- **状态字段**：
  - `state["req_list"]`：当前需求清单（JSON数组）
  - `state["reference_srs"]`：参考SRS文档（文本字符串，作为评分标准）
  - `state["frozen_ids"]`：已冻结需求ID列表（不参与评分）
  - `state["iteration"]`：当前迭代轮数

- **LLM配置**：
  - 模型：从环境变量 `OPENAI_MODEL` 读取（默认：`gpt-4o-mini`）
  - 温度：`0.2`（可通过 `OPENAI_TEMP_REQCLARIFY` 环境变量覆盖）
  - 重试次数：最多3次

### Prompt设计

```python
'You are the "Requirement Clarification Agent (ReqClarify)", scoring "unfrozen" requirements item by item from the **requester/acceptor** perspective;
using "Reference SRS" as the sole standard, anchoring terminology/units/thresholds/role names, prohibiting introducing new requirements or rewriting the original text.

[Scoring Criteria]
+2 (Strong Adoption): Consistent with or better than reference SRS; semantically complete, testable, clear boundaries, no ambiguous words.
+1 (Adoption): Basically consistent, only missing a few verifiable details (exceptions/permissions/observability, etc.).
0 (Neutral): Insufficient information or too many uncertain assumptions, need to condition or supplement verification criteria before re-evaluation.
-1 (Non-adoption): Obvious gaps or deviations in role/scope/trigger/result/failure strategy/consistency, etc., should be rewritten or replaced.
-2 (Strong Non-adoption): Severely conflicts with goals/constraints/compliance or cross-domain; **should be directly removed, and prohibited from re-mentioning or near-synonym rewriting in subsequent iterations**.

[Consistency Constraints]
• Must score each unfrozen requirement in the input item by item; order and length must match the input; ids must correspond one-to-one.
• Output only one array wrapped in ```json fence: [{"id":"FR-01","score":1,"reason":"..."}].
• reason should be concise and auditable (≤50 characters).

---

Requirements to Score (unfrozen) List:
```json
{unfrozen_list_json}
```

Reference SRS:
```text
{reference_srs}
```

Please output scoring JSON array (wrapped with ```json```):'
```

**Prompt特点**：
- 评分标准：+2（强采纳）到-2（强不采纳）
- 以参考SRS为唯一标准，锚定术语/单位/阈值/角色名
- 禁止引入新需求或重写原文
- 要求逐条评分，ID一一对应

### 操作：ReqClarify

**函数位置**：`srs_generator_v1.py:562-687`

**执行流程**：

1. **初始化LLM**：
   ```python
   llm = get_llm_for("ReqClarify")  # temperature=0.2
   ```

2. **过滤未冻结需求**：
   ```python
   frozen_ids_set = set(state["frozen_ids"])
   unfrozen_list = [
       item for item in state["req_list"] 
       if item["id"] not in frozen_ids_set
   ]
   ```

3. **提前退出检查**：
   - 如果 `unfrozen_list` 为空，跳过评分，直接返回

4. **构建Prompt**：
   - 序列化未冻结列表为JSON字符串
   - 使用 `user_template.format(unfrozen_list_json=..., reference_srs=...)` 填充

5. **调用LLM并解析JSON**：
   - 使用 `invoke_with_json_retry(llm, messages, max_retries=3)`
   - 期望输出格式：`[{"id": "FR-01", "score": 1, "reason": "..."}, ...]`

6. **规范化评分映射**：
   ```python
   scores_map: Dict[str, int] = {}
   for item in evaluations:
       rid = str(item.get("id"))
       sc_val = item.get("score")
       try:
           sc = int(sc_val)
       except Exception:
           continue
       if rid:
           scores_map[rid] = sc
   state["scores"] = scores_map
   ```

7. **处理移除项（-2分）**：
   ```python
   to_remove_now = [rid for rid, sc in scores_map.items() if sc == -2]
   if to_remove_now:
       for rid in to_remove_now:
           if rid not in state["removed_ids"]:
               state["removed_ids"].append(rid)
   ```

8. **冻结策略（最高正向分≥+1）**：
   ```python
   if scores_map:
       max_score = max(scores_map.values())
       if max_score >= 1:
           top_ids = [rid for rid, sc in scores_map.items() if sc == max_score]
           for rid in top_ids:
               if rid not in state["frozen_ids"]:
                   state["frozen_ids"].append(rid)
                   state["frozen_reqs"][rid] = req_map.get(rid, "")
   ```

9. **更新迭代计数**：
   - `state["iteration"] += 1`

### 输出

- **状态更新**：
  - `state["scores"]`：评分字典 `{id: score}`
    - 评分范围：+2, +1, 0, -1, -2
    - 仅包含未冻结需求的评分
  - `state["removed_ids"]`：追加得分为-2的需求ID
  - `state["frozen_ids"]`：追加最高正向分（≥+1）的需求ID
  - `state["frozen_reqs"]`：记录冻结需求的内容 `{id: content}`
  - `state["iteration"]`：递增1

- **日志记录**：
  - `state["logs"]` 中追加一条记录

- **控制台输出**：
  - `[日志] ReqClarify：第 X 轮，对需求逐条评分（排除冻结项）`
  - `[日志] ReqClarify：标记移除 N 项（-2），累计移除 M 项`
  - `[日志] ReqClarify：本轮最高正向分 = X，新增冻结 N 项；累计冻结 M 项`
  - `[日志] ReqClarify：评分记录完成（仅未冻结项）`

---

## （四）文档生成智能体（DocGenerate）

### 输入

- **状态字段**：
  - `state["req_list"]`：最终需求清单（JSON数组）
  - `state["removed_ids"]`：已移除需求ID列表（用于过滤）

- **LLM配置**：
  - 模型：从环境变量 `OPENAI_MODEL` 读取（默认：`gpt-4o-mini`）
  - 温度：`0.1`（可通过 `OPENAI_TEMP_DOCGENERATE` 环境变量覆盖）
  - 流式输出：启用（`streaming=True`）
  - 重试次数：最多5次

### Prompt设计

```python
'You are the "Document Generation Agent (DocGenerate)". Please convert the optimized requirement list into
a Software Requirements Specification (SRS) following IEEE Std 830-1998, with Markdown as the output medium.
• Language should be formal, unambiguous, and testable; avoid uncertain expressions like "possibly/probably/TBD".
Final Requirement List (JSON array):
```json
{effective_req_list_json}
```

Please output a Markdown version SRS following the basic format of IEEE Std 830-1998:'
```

**Prompt特点**：
- 遵循IEEE Std 830-1998标准格式
- 要求正式、无歧义、可测试的语言
- 避免不确定表达（如"possibly/probably/TBD"）
- 输出Markdown格式

### 操作：DocGenerate

**函数位置**：`srs_generator_v1.py:690-784`

**执行流程**：

1. **初始化流式输出**：
   ```python
   stream_handler = StreamingPrinter()  # 实时打印回调
   llm = get_llm_for("DocGenerate", streaming=True, callbacks=[stream_handler])
   ```

2. **过滤有效需求**（排除移除项）：
   ```python
   effective_req_list = [
       it for it in state["req_list"]
       if it["id"] not in set(state.get("removed_ids", []))
   ]
   ```

3. **构建Prompt**：
   - 序列化有效需求列表为JSON字符串
   - 使用 `user_template.format(effective_req_list_json=...)` 填充

4. **流式调用LLM（带重试）**：
   ```python
   max_retries = 5
   for attempt in range(max_retries):
       try:
           resp = llm.invoke(messages)
           break
       except Exception as e:
           if attempt < max_retries - 1:
               wait_time = 2 ** attempt  # 指数退避
               time.sleep(wait_time)
               # 重新创建stream_handler和llm
               stream_handler = StreamingPrinter()
               llm = get_llm_for("DocGenerate", streaming=True, callbacks=[stream_handler])
           else:
               raise Exception(...)
   ```

5. **提取完整文本**：
   ```python
   raw_content = resp.content
   full_text = (
       raw_content if isinstance(raw_content, str)
       else "".join(str(part) for part in raw_content)
       if isinstance(raw_content, list)
       else str(raw_content) or ""
   )
   if not full_text:
       full_text = stream_handler.get_text()  # 从缓冲区获取
   ```

6. **更新状态**：
   - `state["srs_output"] = full_text`
   - `state["srs_stream_printed"] = True`

### 输出

- **状态更新**：
  - `state["srs_output"]`：完整的Markdown格式SRS文档（字符串）
  - `state["srs_stream_printed"]`：设置为 `True`（标记已流式打印）

- **日志记录**：
  - `state["logs"]` 中追加一条记录（包含输入消息，但 `raw_output` 为 `None`，因为已流式打印）

- **控制台输出**：
  - `====== 实时 Markdown SRS 输出（流式） ======`
  - 流式打印的Markdown内容（实时显示）
  - `====== 流式输出结束 ======`
  - `[日志] DocGenerate：流式输出完成`

---

## （五）SRS需求匹配度评估智能体

### 输入

- **函数参数**：
  - `standard_srs`：标准参考SRS文档（文本字符串）
  - `evaluated_srs`：待评估SRS文档（文本字符串）
  - `model`：使用的模型名称（可选，默认从环境变量读取）
  - `temperature`：模型温度参数（默认：`0.2`）

- **环境变量**：
  - `OPENAI_API_KEY`：必需，OpenAI API密钥
  - `OPENAI_EVALUATION_MODEL`：优先使用（可选）
  - `OPENAI_MODEL`：回退模型（可选，默认：`gpt-4o-mini`）
  - `OPENAI_BASE_URL`：API基础URL（可选，用于兼容其他OpenAI兼容API）

### Prompt设计

```python
f"""You are a strict SRS requirement review scoring assistant.

Task: Using the "reference requirement document" as ground truth, compare it with the "user requirement document", output only structured score JSON, do not output any explanatory text, tables, code blocks, or extra fields.

Scoring Requirements (all are 0~1 floating point numbers, 4 decimal places precision is sufficient):

1. coverage (Coverage)
   Definition: Items in reference requirements that are "fully covered" by user requirements score 1 point, "partially covered" score 0.5 points, "not covered" score 0 points;
   coverage = (fully covered items count + 0.5 × partially covered items count) / total reference items count.

2. completeness (Completeness)
   Dimensions: Whether it covers main primary flows, key exception flows, boundary conditions, data and state, acceptance criteria, compliance/security, etc.
   High score means "covered functions are written in sufficient detail and are usable".

3. consistency (Consistency)
   Dimensions: Whether user requirements are internally self-consistent, whether there are conflicts or self-contradictory descriptions with reference requirements.
   High score means "no or very few obvious conflicts".

4. testability (Testability)
   Dimensions: Whether requirements are verifiable (whether there are clear trigger conditions, expected results, measurable standards, avoiding unverifiable general terms).
   High score means "most items can be used to design test cases".

5. clarity (Clarity)
   Dimensions: Whether there are vague subjective descriptions like "fast", "reasonable", "friendly", "as much as possible", whether there are composite requirements or ambiguities.
   High score means "clear expression, appropriate granularity, few ambiguities".

6. traceability (Traceability)
   Dimensions: Whether user requirements can have clear mapping back to reference requirements (numbering/title/semantics), whether it is easy to build RTM (Requirements Traceability Matrix).
   High score means "most requirements can clearly correspond to sources".

7. scope_discipline (Scope Management)
   Dimensions: Whether it introduces many requirements beyond reference scope causing scope creep; whether there are clear boundaries for new scope.
   High score means "scope converges well, few new points and clearly marked boundaries".

8. by_category (Score by Category)
   - functional: Overall quality of functional requirements (comprehensive performance of the above metrics on the functional requirements subset).
   - non_functional: Quality related to non-functional requirements (performance, security, reliability, availability, etc.).
   - constraints: Coverage and clarity of mandatory items such as constraints/interfaces/compliance.

Output Format (must strictly satisfy):
- Output only one JSON object, prohibit outputting any Markdown, comments, natural language explanations, or code block markers.
- JSON top-level keys only include:
  - "metrics"

Where:
"metrics" : {{
  "coverage": <float>,
  "completeness": <float>,
  "consistency": <float>,
  "testability": <float>,
  "clarity": <float>,
  "traceability": <float>,
  "scope_discipline": <float>,
  "by_category": {{
    "functional": <float>,
    "non_functional": <float>,
    "constraints": <float>
  }}
}}

Constraints:
- Do not output RTM, do not output gaps, do not output rewriting suggestions, do not output Summary, do not add any other fields.
- If information is insufficient, please truthfully reflect uncertainty in the scores (give conservative scores), but still must return all the above fields, prohibit outputting explanatory text.
- Ensure the return value is valid JSON (double quotes, comma positions, boolean/numeric formats are all correct).

[Reference Requirements]
{standard_srs}

[User Requirements]
{evaluated_srs}
"""
```

**Prompt特点**：
- 7个核心指标：覆盖率、完整度、一致性、可验证性、明确性、可追溯性、范围管理
- 分类评分：功能需求、非功能需求、约束
- 严格JSON输出格式，禁止额外文本
- 所有指标为0~1浮点数，保留4位小数

### 操作：evaluate_srs

**函数位置**：`srs_evaluation.py:116-326`

**执行流程**：

1. **初始化OpenAI客户端**：
   ```python
   api_key = os.environ.get("OPENAI_API_KEY")
   client = OpenAI(api_key=api_key, base_url=base_url)
   ```

2. **确定模型**：
   ```python
   model = model or os.environ.get("OPENAI_EVALUATION_MODEL") or os.environ.get("OPENAI_MODEL", "gpt-4o-mini")
   ```

3. **构建Prompt**：
   - 使用f-string格式化，填充 `standard_srs` 和 `evaluated_srs`

4. **调用API（带重试）**：
   ```python
   max_retries = 5
   for retry_count in range(max_retries + 1):  # 0到5，共6次尝试
       try:
           response = client.chat.completions.create(
               model=model,
               messages=[{"role": "user", "content": user_prompt}],
               temperature=temperature
           )
           break
       except Exception as e:
           if retry_count < max_retries:
               delay = 2 * (2 ** retry_count)  # 2s, 4s, 8s, 16s, 32s
               time.sleep(delay)
           else:
               raise RuntimeError(...)
   ```

5. **多方法JSON解析**：
   ```python
   content = response.choices[0].message.content.strip()
   parsed_result = None
   
   # 方法1: 直接解析
   try:
       parsed_result = json.loads(content)
   except json.JSONDecodeError:
       pass
   
   # 方法2: 从markdown代码块提取
   if parsed_result is None:
       json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', content, re.DOTALL)
       if json_match:
           try:
               parsed_result = json.loads(json_match.group(1))
           except json.JSONDecodeError:
               pass
   
   # 方法3: 提取第一个{到最后一个}之间的内容
   if parsed_result is None:
       first_brace = content.find('{')
       last_brace = content.rfind('}')
       if first_brace != -1 and last_brace > first_brace:
           json_str = content[first_brace:last_brace + 1]
           try:
               parsed_result = json.loads(json_str)
           except json.JSONDecodeError:
               pass
   ```

6. **验证和计算综合评分**：
   ```python
   if parsed_result is not None:
       if "metrics" in parsed_result:
           # 计算综合评分（简单平均和加权平均）
           comprehensive_scores = calculate_comprehensive_score(parsed_result)
           if comprehensive_scores:
               parsed_result.update(comprehensive_scores)
   ```

7. **综合评分计算**（`calculate_comprehensive_score`函数）：
   ```python
   metric_weights = {
       "coverage": 0.20,
       "completeness": 0.20,
       "testability": 0.20,
       "traceability": 0.15,
       "consistency": 0.10,
       "clarity": 0.10,
       "scope_discipline": 0.05
   }
   
   # 简单平均
   simple_avg = sum(metric_values.values()) / len(metric_values)
   
   # 加权平均
   weighted_sum = sum(metrics[name] * weight for name, weight in metric_weights.items() if name in metric_values)
   weighted_avg = weighted_sum / total_weight
   
   return {
       "Comprehensive_Score_Simple": round(simple_avg, 4),
       "Comprehensive_Score_Weighted": round(weighted_avg, 4)
   }
   ```

### 输出

- **返回值**：`Dict[str, Any]`，包含以下字段：

  - `metrics`：指标字典
    ```json
    {
      "coverage": <float>,
      "completeness": <float>,
      "consistency": <float>,
      "testability": <float>,
      "clarity": <float>,
      "traceability": <float>,
      "scope_discipline": <float>,
      "by_category": {
        "functional": <float>,
        "non_functional": <float>,
        "constraints": <float>
      }
    }
    ```
  
  - `Comprehensive_Score_Simple`：简单平均综合评分（7个指标等权）
  - `Comprehensive_Score_Weighted`：加权平均综合评分（偏重落地性）
    - 权重：coverage/completeness/testability各20%，traceability 15%，consistency/clarity各10%，scope_discipline 5%

- **错误情况**：
  - 如果JSON解析失败，返回：
    ```json
    {
      "error": "模型输出的不是有效JSON",
      "raw_output": "<前500字符预览>",
      "raw_output_length": <长度>
    }
    ```

---

## 状态流转机制

### GraphState结构

```python
class GraphState(TypedDict):
    user_input: str                    # 用户自然语言需求
    reference_srs: str                  # 参考SRS文档（用于评分标准）
    req_list: List[Dict[str, Any]]     # 需求清单 [{id, content}, ...]
    scores: Dict[str, int]             # 需求评分 {id: score}
    logs: List[Dict[str, Any]]         # LLM交互日志
    iteration: int                      # 当前迭代轮数
    max_iterations: int                 # 最大迭代轮数（默认5）
    srs_output: str                    # 最终SRS文档（Markdown）
    srs_stream_printed: bool            # 是否已流式打印
    frozen_ids: List[str]              # 已冻结需求ID列表
    frozen_reqs: Dict[str, str]        # 冻结需求内容 {id: content}
    removed_ids: List[str]             # 已移除需求ID列表
    ablation_mode: Optional[str]       # 消融实验模式
```

### 冻结与移除机制

1. **冻结机制**：
   - 触发条件：ReqClarify评分后，最高正向分（≥+1）的所有需求
   - 效果：冻结项在后续ReqExplore和ReqClarify中只读，不参与修改
   - 存储：`frozen_ids`（ID列表）+ `frozen_reqs`（内容字典）

2. **移除机制**：
   - 触发条件：ReqClarify评分中得分为-2（强不采纳）的需求
   - 效果：移除项不参与后续处理，不出现在最终SRS中
   - 存储：`removed_ids`（ID列表）

3. **状态保护**：
   - ReqExplore：仅处理未冻结且未移除项，输出中不包含冻结/移除ID
   - ReqClarify：仅对未冻结项评分
   - DocGenerate：生成文档时排除移除项

---

## 错误处理与重试机制

### LLM调用重试

所有智能体均使用 `invoke_with_json_retry()` 函数，特点：
- **最大重试次数**：ReqParse/ReqExplore/ReqClarify为3次，DocGenerate为5次
- **退避策略**：指数退避，等待时间 = 2^attempt 秒
- **重试条件**：
  - JSON解析失败（ValueError）
  - API调用异常（网络错误、HTTP状态码错误等）
- **日志记录**：每次重试都会记录日志

### JSON解析容错

- **ReqParse/ReqExplore/ReqClarify**：使用 `extract_first_json()` 函数
  - 优先匹配 ````json ... ``` 代码围栏
  - 失败则尝试裸JSON（大括号/中括号匹配）
  - 全部失败则抛出异常并显示输出预览

- **评估智能体**：使用4种方法逐步尝试
  1. 直接JSON解析
  2. 从markdown代码块提取
  3. 提取第一个{到最后一个}之间的内容
  4. 正则匹配所有可能的JSON对象

---

## 环境变量配置

| 变量名 | 说明 | 默认值 | 使用位置 |
|--------|------|--------|----------|
| `OPENAI_API_KEY` | OpenAI API密钥 | 必需 | 所有智能体 |
| `OPENAI_MODEL` | 使用的模型名称 | `gpt-4o-mini` | 所有智能体 |
| `OPENAI_BASE_URL` | API基础URL | 无 | 所有智能体（兼容私有部署） |
| `OPENAI_TEMP_REQPARSE` | ReqParse温度 | `0.2` | ReqParse |
| `OPENAI_TEMP_REQEXPLORE` | ReqExplore温度 | `0.6` | ReqExplore |
| `OPENAI_TEMP_REQCLARIFY` | ReqClarify温度 | `0.2` | ReqClarify |
| `OPENAI_TEMP_DOCGENERATE` | DocGenerate温度 | `0.1` | DocGenerate |
| `OPENAI_EVALUATION_MODEL` | 评估模型（优先） | 无 | 评估智能体 |

---

## 总结

本系统通过5个智能体的协作，实现了从自然语言需求到标准化SRS文档的自动化生成流程。关键设计特点：

1. **迭代优化**：ReqExplore ↔ ReqClarify 循环，逐步优化需求质量
2. **状态保护**：冻结和移除机制确保已确定需求不再变动
3. **评分驱动**：基于评分的差异化挖掘策略，提高需求完整性
4. **标准输出**：遵循IEEE Std 830-1998标准，生成规范的SRS文档
5. **质量评估**：独立的评估智能体，提供7维度量化评分

所有智能体均具备完善的错误处理和重试机制，确保系统稳定运行。

